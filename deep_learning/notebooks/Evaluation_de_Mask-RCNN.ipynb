{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation du modèle Mask R-CNN\n",
    "\n",
    "Ce notebook complète l'article publié sur le blog de Makina Corpus à propos de l'**extraction d'objets pour la cartographie par deep-learning**, et plus particulièrement la partie concernant l'[évaluation du modèle](https://makina-corpus.com/blog/metier/2020/extraction-dobjets-pour-la-cartographie-par-deep-learning-evaluation-du-modele)\n",
    "\n",
    "Ce notebook fait suite au notebook [Utilisation_de_Mask-RCNN](https://github.com/makinacorpus/tutorials/blob/master/deep_learning/notebooks/Utilisation_de_Mask-RCNN.ipynb) qui présente comment utiliser l'implémentation du modèle Mask R-CNN (disponible ici : [Matterport-Mask RCNN](https://github.com/matterport/Mask_RCNN/blob/master)). Il nécessite les mêmes installations :\n",
    "* **`tensorflow`** et **`keras`**\n",
    "* **`pycocotools`** (voir commande ci-dessous)\n",
    "```console\n",
    "pip install git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#IMPORT LIBRAIRIES\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "#pycocotools\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "#mrcnn\n",
    "from mrcnn import visualize\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des variables et des dossiers nécessaires \n",
    "\n",
    "Nous utiliserons dans ce tutoriel les poids du modèle déjà entraîné : [submitted_weights.h5](https://gitlab.com/LaurentS/crowdai-mapping-challenge-mask-rcnn/-/blob/master/submitted_weights.h5) de cette [solution](https://gitlab.com/LaurentS/crowdai-mapping-challenge-mask-rcnn)\n",
    " au challenge Crowd AI Mapping challenge.\n",
    " \n",
    "Le dossier `\"logs\"` correspond au dossier dans lequel les différents poids obtenus par l'entraînement du modèle seront enregistrés. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparation des chemins\n",
    "ROOT_DIR = \"\"\n",
    "PRETRAINED_MODEL_PATH =\"submitted_weights.h5\"\n",
    "LOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation du modèle en mode inférence \n",
    "\n",
    "Avec le fichier contenant les poids , il est possible de réaliser des évaluations. Pour cela nous devons configurer le modèle et l'intialiser en mode inférence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration du modèle pour l'évaluation\n",
    "\n",
    "La configuration du modèle va varier selon le mode de fonctionnement du modèle. L'implémentation de Mask R-CNN contient déjà une classe pour la configuration du modèle : la classe `Config` du fichier `mrcnn.config`.\n",
    "\n",
    "Il suffit donc de créer une classe qui hérite de celle-ci pour configurer le modèle à notre utilisation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.999\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           Buildings_Detection\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CONFIGURATION DU MODELE POUR LA PREDICTION ET L EVALUATION\n",
    "\n",
    "class Buildings_Inference(Config):\n",
    "    NAME = \"Buildings_Detection\"\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 1  # Nous avons une classe \"Buildings\" , le background étant la classe par défaut , cela donne : 1 Background + 1 Building\n",
    "    IMAGE_MAX_DIM=256\n",
    "    IMAGE_MIN_DIM=256\n",
    "    DETECTION_MIN_CONFIDENCE = 0.90 # c'est le seuil de confiance (score de prédiction) à partir du quel on veut que le modèle valide la détection de l'objet.\n",
    "\n",
    "#initialisation de la configuration du modèle   \n",
    "config = Buildings_Inference()\n",
    "\n",
    "#visualisation de la configuration\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du modèle en mode inférence \n",
    "\n",
    "Nous initialisons le modèle en mode inférence avec la configuration définie précédemment.\n",
    "Comme nous avons à notre disposition un modèle déjà entraîné , nous pouvons charger les poids de ce modèle pour la prédiction appliquée à nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TELECHARGEMENT DU MODELE ET CHARGEMENT DES POIDS DU MODELE PRE-ENTRAINE\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config) \n",
    "model_path = PRETRAINED_MODEL_PATH\n",
    "model.load_weights(model_path, by_name=True) #téléchargement des poids du modèle déja entrainé\n",
    "class_names = ['BG', 'buildings'] # Liste contenant le nom des catégories, permettant la visualisation du nom des catégories détectées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration de la classe `BuildingDataset()`\n",
    "\n",
    "Pour se préparer à l'évaluation du modèle, il faut permettre au modèle de parcourir notre jeu de vérité terrain : pour cela, nous utilisons la classe `BuildingDataset()`.\n",
    "Elle hérite de la classe `Dataset()` de `mrcnn.utils` et s'adapte à notre vérité terrain.\n",
    "\n",
    "Sa principale fonctionnalité est de charger le fichier d'annotations et d'en extraire les informations qui y sont contenues ainsi que de permettre la conversion des masques dans un format pouvant être lu et utilisé par le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLASSE PERMETTANT LA CONFIGURATION DU DATASET\n",
    "\n",
    "class BuildingDataset(utils.Dataset):\n",
    "    \n",
    "    \"\"\" Part 1 : Download Annotation file \"\"\"\n",
    "    \n",
    "    def load_dataset(self,annotation_file,image_dir,return_coco=True, limit=None):\n",
    "        \"\"\"\n",
    "        return COCO dataset\n",
    "        \"\"\"\n",
    "        # COCO DATASET\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.image_dir =image_dir\n",
    "        \n",
    "        # DATASET INFORMATIONS\n",
    "        # Load all classes (Only Building in this version)\n",
    "        classIds = self.coco.getCatIds()\n",
    "        # Load all images\n",
    "        image_ids = list(self.coco.imgs.keys())[:limit]\n",
    "        print(len(image_ids), 'images loaded')\n",
    "        # register classes\n",
    "        for _class_id in classIds:\n",
    "            self.add_class(\"buildings_data\", _class_id, self.coco.loadCats(_class_id)[0][\"name\"])\n",
    "        # Register Images\n",
    "        for _img_id in image_ids:\n",
    "            assert(os.path.exists(os.path.join(image_dir, self.coco.imgs[_img_id]['file_name'])))\n",
    "            self.add_image(\n",
    "                \"buildings_data\", image_id=_img_id,\n",
    "                path=os.path.join(image_dir, self.coco.imgs[_img_id]['file_name']),\n",
    "                width=self.coco.imgs[_img_id][\"width\"],\n",
    "                height=self.coco.imgs[_img_id][\"height\"],\n",
    "                annotations=self.coco.loadAnns(self.coco.getAnnIds(\n",
    "                                            imgIds=[_img_id],\n",
    "                                            catIds=classIds,\n",
    "                                            iscrowd=0)))\n",
    "\n",
    "        if return_coco:\n",
    "            return self.coco\n",
    "        \n",
    "        \n",
    "    \"\"\" Part : Download Mask \"\"\"    \n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\" Loads instance mask for a given image\n",
    "              This function converts mask from the coco format to a\n",
    "              a bitmap [height, width, instance]\n",
    "            Params:\n",
    "                - image_id : reference id for a given image\n",
    "\n",
    "            Returns:\n",
    "                masks : A bool array of shape [height, width, instances] with\n",
    "                    one mask per instance\n",
    "                class_ids : a 1D array of classIds of the corresponding instance masks)\n",
    "        \"\"\"\n",
    "\n",
    "        image_info = self.image_info[image_id]\n",
    "        assert image_info[\"source\"] == \"buildings_data\"\n",
    "\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        annotations = self.image_info[image_id][\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.map_source_class_id(\n",
    "                \"buildings_data.{}\".format(annotation['category_id']))\n",
    "            if class_id:\n",
    "                m = self.annToMask(annotation,  image_info[\"height\"],\n",
    "                                                image_info[\"width\"])\n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "\n",
    "                # Ignore the notion of \"is_crowd\" as specified in the coco format\n",
    "                # as we donot have the said annotation in the current version of the dataset\n",
    "\n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "        # Pack instance masks into an array\n",
    "        if class_ids:\n",
    "            mask = np.stack(instance_masks, axis=2)\n",
    "            class_ids = np.array(class_ids, dtype=np.int32)\n",
    "            \n",
    "            \n",
    "            return mask,class_ids  \n",
    "\n",
    "        else:\n",
    "            # return an empty mask if no instance for the given image.\n",
    "            mask = np.empty([0, 0, 0])\n",
    "            class_ids = np.empty([0], np.int32)\n",
    "            return mask, class_ids\n",
    "\n",
    "   \n",
    "     \n",
    "    \"\"\"Part : Tools to convert masks \"\"\"\n",
    "    \n",
    "    def annToRLE(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE to RLE.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        segm = ann['segmentation']\n",
    "        if isinstance(segm, list):\n",
    "            # polygon -- a single object might consist of multiple parts\n",
    "            # we merge all parts into one mask rle code\n",
    "            rles = maskUtils.frPyObjects(segm, height, width)\n",
    "            rle = maskUtils.merge(rles)\n",
    "        elif isinstance(segm['counts'], list):\n",
    "            # uncompressed RLE\n",
    "            rle = maskUtils.frPyObjects(segm, height, width)\n",
    "        else:\n",
    "            # rle\n",
    "            rle = ann['segmentation']\n",
    "        return rle\n",
    "\n",
    "    def annToMask(self, ann, height, width):\n",
    "        \"\"\"\n",
    "        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n",
    "        :return: binary mask (numpy 2D array)\n",
    "        \"\"\"\n",
    "        rle = self.annToRLE(ann, height, width)\n",
    "        m = maskUtils.decode(rle)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialisation du jeu de données pour l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPARATION DU DATASET\n",
    "\n",
    "# fchiers validation\n",
    "annot_file_val=\"fichier annotation pour la validation.json\"\n",
    "img_directory_val=\"Dossier images pour la validation\"\n",
    "\n",
    "#validation dataset\n",
    "data_val= BuildingDataset()\n",
    "coco_data_val = data_val.load_dataset(annot_file_val,img_directory_val,return_coco=True)\n",
    "data_val.prepare() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions permettant l'évaluation\n",
    "\n",
    "L'évaluation va permettre de lancer un ensemble de prédictions et de comparer les résultats aux annotations de notre vérité terrain. Pour ce faire, nous utilisons les fonctions mises au point pour le dataset COCO.\n",
    "\n",
    "Ces fonctions sont issues de https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'évaluation \n",
    "\n",
    "def evaluate_coco(model, dataset, coco, eval_type=\"bbox\", limit=0, image_ids=None):\n",
    "    \"\"\"Runs official COCO evaluation.\n",
    "    dataset: A Dataset object with vallidation data\n",
    "    eval_type: \"bbox\" or \"segm\" for bounding box or segmentation evaluation\n",
    "    limit: if not 0, it's the number of images to use for evaluation\n",
    "    \"\"\"\n",
    "    # Pick COCO images from the dataset\n",
    "    image_ids = image_ids or dataset.image_ids\n",
    "\n",
    "    # Limit to a subset\n",
    "    if limit:\n",
    "        image_ids = image_ids[:limit]\n",
    "\n",
    "    # Get corresponding COCO image IDs.\n",
    "    coco_image_ids = [dataset.image_info[id][\"id\"] for id in image_ids]\n",
    "    t_prediction = 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, image_id in enumerate(image_ids):\n",
    "        # Load image\n",
    "        image = dataset.load_image(image_id)\n",
    "\n",
    "        # Run detection\n",
    "        t = time.time()\n",
    "        print(\"=\"*100)\n",
    "        print(\"Image shape \", image.shape)\n",
    "        r = model.detect([image])\n",
    "        r = r[0]\n",
    "        t_prediction += (time.time() - t)\n",
    "        print(\"Prediction time : \", (time.time() - t))\n",
    "        # Convert results to COCO format\n",
    "        image_results = build_coco_results(dataset, coco_image_ids[i:i + 1],\n",
    "                                           r[\"rois\"], r[\"class_ids\"],\n",
    "                                           r[\"scores\"], r[\"masks\"])\n",
    "        print(\"Number of detections : \", len(r[\"rois\"]))\n",
    "        print(\"Classes Predicted : \", r[\"class_ids\"])\n",
    "        print(\"Scores : \", r[\"scores\"])\n",
    "        results.extend(image_results)\n",
    "\n",
    "    # Load results. This modifies results with additional attributes.\n",
    "    coco_results = coco.loadRes(results)\n",
    "\n",
    "    # Evaluate\n",
    "    cocoEval = COCOeval(coco, coco_results, eval_type)\n",
    "    cocoEval.params.imgIds = coco_image_ids\n",
    "    cocoEval.evaluate()\n",
    "    cocoEval.accumulate()\n",
    "    cocoEval.summarize()\n",
    "    \n",
    "\n",
    "    print(\"Prediction time: {}. Average {}/image\".format(\n",
    "        t_prediction, t_prediction / len(image_ids)))\n",
    "    print(\"Total time: \", time.time() - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation des résultats de la prédiction du modèle en format COCO.\n",
    "def build_coco_results(dataset, image_ids, rois, class_ids, scores, masks):\n",
    "    \"\"\"Arrange resutls to match COCO specs in http://cocodataset.org/#format\n",
    "    \"\"\"\n",
    "    # If no results, return an empty list\n",
    "    if rois is None:\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for image_id in image_ids:\n",
    "        # Loop through detections\n",
    "        for i in range(rois.shape[0]):\n",
    "            class_id = class_ids[i]\n",
    "            score = scores[i]\n",
    "            bbox = np.around(rois[i], 1)\n",
    "            masks=masks.astype(np.uint8)\n",
    "            mask = masks[:, :, i]\n",
    "            y1, x1, y2, x2 = bbox\n",
    "            result = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": dataset.get_source_class_id(class_id, \"buildings_data\"),\n",
    "                \"bbox\": [y1, x1, y2, x2 ],\n",
    "                \"score\": score,\n",
    "                \"segmentation\": maskUtils.encode(np.asfortranarray(mask))\n",
    "            }\n",
    "            results.append(result)\n",
    "         \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lancement de l'évaluation\n",
    "\n",
    "Nous pouvons ainis lancer une évaluation du modèle sur notre jeu de données. L'option `eval_type` permet de définir ce que nous cherchons à évaluer. Dans cet exemple, l'option `\"segm\"` permet de lancer une évaluation sur la performance du modèle à prédire les masques associés aux bâtiments. Pour évaluer le modèle sur les boites englobantes prédites, il faut remplacer cette option par `\"bbox\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Evaluation du modèle avec 1000 images\n",
    "evaluate_coco(model,dataset_val,val_coco, eval_type=\"segm\",limit=1000) données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Exemple d'un output obtenu suite à l'évalution :\n",
    " \n",
    " ```python\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.207\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.402\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.193\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.072\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.309\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.291\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.323\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.022\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.128\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.475\n",
    "Prediction time: 2387.38641166687. Average 2.38738641166687/image\n",
    "Total time:  2390.99524474144\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le format de ce résultat d'évaluation est documenté dans l'[article en ligne sur le blog de Makina Corpus](https://makina-corpus.com/blog/metier/2020/extraction-dobjets-pour-la-cartographie-par-deep-learning-evaluation-du-modele).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep_env)",
   "language": "python",
   "name": "deep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
